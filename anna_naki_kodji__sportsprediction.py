# -*- coding: utf-8 -*-
"""Anna Naki Kodji._SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E9qzfrsUS5R4-IwTzWObKq8ftl-4Bt0K

Importing Libraries
"""

import pandas as pd
import joblib
import pickle
from sklearn.impute import SimpleImputer
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression  .
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor, StackingRegressor
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import RFE

male_players=pd.read_csv(r'C:\Users\Anna Kodji\Downloads\male_players (legacy).csv')

male_players=pd.DataFrame(male_players)

male_players

male_players.head()

# List of columns to drop
columns_todrop = [
    'player_url', 'player_id', 'fifa_version', 'dob', 'long_name', 'club_team_id', 'club_loaned_from',
    'nationality_id', 'nation_team_id', 'player_tags', 'player_traits', 'real_face',
    'player_face_url', 'club_jersey_number', 'nation_jersey_number', 'club_name', 'fifa_update_date', 'league_id',
    'league_name', 'fifa_update', 'league_level', 'work_rate',
    'club_position', 'club_joined_date', 'club_contract_valid_until_year', 'nationality_name',
    'nation_position'
]

#dropping columns that are not needed
male_players=male_players.drop(columns=columns_todrop)

for columns in male_players.columns:
  print(columns)

#removing columns with high percentage of missing values
threshold = len(male_players) * 0.7
male_players =male_players.dropna(thresh=threshold, axis=1)

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

#checking for Na's in the dataset
male_players.isna().any()

L=[]
L_less=[]
for i in male_players.columns:
    if((male_players[i].isnull().sum())<(0.4*(male_players.shape[0]))):
        L.append(i)
    else:
        L_less.append(i)
df=[L]

#filling missing values
numerical_columns = male_players.select_dtypes(include=['number']).columns
for col in numerical_columns:
    mean_value =male_players[col].mean()
    male_players[col].fillna(mean_value, inplace=True)

# Fill NaN values in categorical columns with the mode
categorical_columns = male_players.select_dtypes(include=['object', 'category']).columns
for col in categorical_columns:
    mode_value = male_players[col].mode()[0]  # mode() returns a Series
    male_players[col].fillna(mode_value, inplace=True)

#checking for Na's in the dataset
male_players.isna().any()

scaler = StandardScaler()

#computing columns with + or -
columns1 = ['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram',
            'lm','lcm','cm','rcm','rm','lwb','ldm', 'cdm','rdm','rwb',
            'lb','lcb','cb','rcb','rb', 'gk']

def convert_position_rating(rating):
    if isinstance(rating, str) and '+' in rating:
        try:
            base, modifier = rating.split('+')
            return int(base) + int(modifier)
        except ValueError:
            return rating  # Return the original if there's an error in splitting or converting
    elif isinstance(rating, str):
        try:
            return int(rating)
        except ValueError:
            return rating  # Return the original if it's not a number or the expected format
    else:
        return rating  # Return the original if it's not a string

for position in columns1:
    if position in male_players.columns:
        male_players[position] = male_players[position].apply(convert_position_rating)

# Convert all remaining non-numeric values to NaN and ensure all are numeric
for position in columns1:
    if position in male_players.columns:
        male_players[position] = pd.to_numeric(male_players[position], errors='coerce')

male_players.head()

#Making the player_position column include just the player's main position
male_players['player_positions']=male_players['player_positions'].apply(lambda x: x.split(',')[0].strip())

unique_positions = male_players['player_positions'].unique()
unique_positions

"""Encoding Categorical Columns"""

male_players = pd.DataFrame(male_players)

#Encode categorical variables
preferred_foot_encoded = pd.get_dummies(male_players['preferred_foot'], prefix='preferred_foot')
body_type_encoded = pd.get_dummies(male_players['body_type'], prefix='body_type')


#player_positions_encoded = pd.get_dummies(male_players['player_positions'], prefix='player_positions')

# Concatenate the new one-hot encoded columns back to the original DataFrame
male_players = pd.concat([male_players, preferred_foot_encoded, body_type_encoded], axis=1)
columns_to_drop = ['player_positions', 'preferred_foot', 'body_type']

# Drop only the columns that exist in the DataFrame
columns_existing_to_drop = [col for col in columns_to_drop if col in male_players.columns]
male_players.drop(columns_existing_to_drop, axis=1, inplace=True)

male_players.head()

print(male_players.columns)

male_players.drop(columns=['short_name'], inplace=True)

"""Finding the correlation between dependent and independent variables

"""

corr_matrix = male_players.corr()
sns.heatmap(corr_matrix)
plt.title('Correlation Matrix')
plt.show()

# Calculate correlation matrix
correlation_matrix = male_players.corr()

# Extract correlations with the dependent variable
correlations_with_target = correlation_matrix['overall'].abs().sort_values(ascending=False)

# Select top correlated features
top_correlated_features = correlations_with_target.head(10)

print("Top correlated features with the target variable:")
print(top_correlated_features)

print("Columns before dropping:")
print(male_players.columns)

# List of columns to drop
attributes_to_drop = [
    'preferred_foot', 'attacking_crossing',
    'attacking_heading_accuracy', 'attacking_short_passing', 'attacking_volleys',
    'skill_dribbling', 'skill_curve', 'skill_fk_accuracy', 'skill_long_passing',
    'movement_agility', 'movement_balance', 'power_shot_power',
    'power_jumping', 'power_long_shots', 'mentality_aggression', 'mentality_interceptions',
    'mentality_vision', 'mentality_penalties', 'defending_marking_awareness',
    'defending_standing_tackle', 'defending_sliding_tackle', 'goalkeeping_diving',
    'goalkeeping_handling', 'goalkeeping_kicking', 'goalkeeping_positioning',
    'goalkeeping_reflexes', 'player_positions_CAM', 'player_positions_CB',
    'player_positions_CDM', 'player_positions_CF', 'player_positions_CM', 'player_positions_GK',
    'player_positions_LB', 'player_positions_LM', 'player_positions_LW', 'player_positions_LWB',
    'player_positions_RB', 'player_positions_RM', 'player_positions_RW', 'player_positions_RWB',
    'player_positions_ST'
]

# Dropping columns
columns_to_drop = [col for col in attributes_to_drop if col in male_players.columns]
male_players.drop(columns=columns_to_drop, axis=1, inplace=True)

# Print columns after dropping
print("\nColumns after dropping:")
print(male_players.columns)

male_players.head()

"""Distribution Of Overall Rating"""

plt.hist(male_players['overall'], bins=30)
plt.title('Distribution of Overall Rating')
plt.show()

male_players.sort_values(by='overall',ascending=False)[["overall","age"]].head(20)

#scaling male_players dataset
male_players_scaled=pd.DataFrame(scaler.fit_transform(male_players), columns=male_players.columns)

male_players_scaled

"""players_22 dataset

"""

players_22=pd.read_csv(r"C:\Users\Anna Kodji\Documents\players_22 rael.csv")

players_22 = pd.DataFrame(players_22)

pd.set_option('display.max_rows', 10)
pd.set_option('display.max_columns', None)

players_22

players_22.head()

players_22.info()

players_22.describe()

"""Data Cleaning"""

#dropping columns that are not needed
players_22 = players_22.drop(columns=columns_to_drop)

players_22

#removing columns with high percentage of missing values
threshold = len(players_22) * 0.7
players_22 =players_22.dropna(thresh=threshold, axis=1)

#checking for Na's in the dataset
players_22.isna().any()

L=[]
L_less=[]
for i in players_22.columns:
    if((players_22[i].isnull().sum())<(0.4*(players_22.shape[0]))):
        L.append(i)
    else:
        L_less.append(i)
df=[L]

#filling missing values
numerical_columns = players_22.select_dtypes(include=['number']).columns
for col in numerical_columns:
    mean_value =players_22[col].mean()
    players_22[col].fillna(mean_value, inplace=True)

# Fill NaN values in categorical columns with the mode
categorical_columns = players_22.select_dtypes(include=['object', 'category']).columns
for col in categorical_columns:
    mode_value = players_22[col].mode()[0]  # mode() returns a Series
    players_22[col].fillna(mode_value, inplace=True)

#checking for Na's in the dataset
players_22.isna().any()

#computing columns with + or -
columns1 = ['ls','st','rs','lw','lf','cf','rf','rw','lam','cam','ram',
            'lm','lcm','cm','rcm','rm','lwb','ldm', 'cdm','rdm','rwb',
            'lb','lcb','cb','rcb','rb', 'gk']

def convert_position_rating(rating):
    if isinstance(rating, str) and '+' in rating:
        try:
            base, modifier = rating.split('+')
            return int(base) + int(modifier)
        except ValueError:
            return rating  # Return the original if there's an error in splitting or converting
    elif isinstance(rating, str):
        try:
            return int(rating)
        except ValueError:
            return rating  # Return the original if it's not a number or the expected format
    else:
        return rating  # Return the original if it's not a string

for position in columns1:
    if position in players_22.columns:
        players_22[position] = players_22[position].apply(convert_position_rating)

# Convert all remaining non-numeric values to NaN and ensure all are numeric
for position in columns1:
    if position in players_22.columns:
        players_22[position] = pd.to_numeric(players_22[position], errors='coerce')

players_22.head()

#Making the player_position column include just the player's main position
players_22['player_positions']=players_22['player_positions'].apply(lambda x: x.split(',')[0].strip())

unique_positions = players_22['player_positions'].unique()
unique_positions

"""Encoding categorical columns"""

players_22 = pd.DataFrame(players_22)

#Encoding categorical variables
preferred_foot_encoded = pd.get_dummies(players_22['preferred_foot'], prefix='preferred_foot')
body_type_encoded = pd.get_dummies(players_22['body_type'], prefix='body_type')

# player_positions_encoded = pd.get_dummies(players_22['player_positions'], prefix='player_positions')

#Concatenate the new one-hot encoded columns back to the original DataFrame
players_22 = pd.concat([players_22, preferred_foot_encoded, body_type_encoded], axis=1)


columns_to_drop = ['player_positions', 'preferred_foot', 'body_type']

# Drop only the columns that exist in the DataFrame
columns_existing_to_drop = [col for col in columns_to_drop if col in players_22.columns]
players_22.drop(columns_existing_to_drop, axis=1, inplace=True)

print(players_22)

"""Exploratory Data Analysis"""

players_22.describe()

players_22.head()

print(players_22.columns)

columns_to_drop = [
    'sofifa_id', 'player_url', 'long_name', 'dob', 'club_team_id',
    'club_name', 'league_name', 'league_level', 'club_position',
    'club_jersey_number', 'club_joined', 'club_contract_valid_until',
    'nationality_id', 'nationality_name', 'player_face_url', 'club_logo_url',
    'club_flag_url', 'nation_flag_url', 'work_rate', 'real_face','release_clause_eur',

]

# Filter the columns to drop based on what exists in the DataFrame
columns_existing_to_drop = [col for col in columns_to_drop if col in players_22.columns]

players_22.drop(columns=columns_existing_to_drop, inplace=True)

players_22.head()

players_22=df = players_22.drop(columns=['short_name'])

"""
Finding the correlation between dependent and indepedent variables"""

# Calculate correlation matrix
correlation_matrix = players_22.corr()

# Extract correlations with the dependent variable
correlations_with_target = correlation_matrix['overall'].abs().sort_values(ascending=False)

# Select top correlated features
top_correlated_features = correlations_with_target.head(10)

print("Top correlated features with the target variable:")
print(top_correlated_features)

print(players_22.columns)

#correlation matrix for players_22 dataset
corr_matrix = players_22.corr()
sns.heatmap(corr_matrix)
plt.title('Correlation Matrix')
plt.show()

"""Distribution of overall rating"""

plt.hist(players_22['overall'], bins=30)
plt.title('Distribution of Overall Rating')
plt.show()

#scaling the players_22 dataset
players_22_scaled=pd.DataFrame(scaler.fit_transform(players_22), columns=players_22.columns)

players_22_scaled

# Separate features (X) and target variable (y) for training data
X_train = male_players_scaled.drop(columns=['overall'])
y_train = male_players['overall']

# Separate features (X) and target variable (y) for testing data
X_test = players_22_scaled.drop(columns=['overall'])
y_test = players_22['overall']

y_test

feature_model = RandomForestRegressor()

pd.set_option('display.max_columns', None)

# If you want to display all rows too
pd.set_option('display.max_rows', None)

# Check for NaNs in X_train
print("Missing values in X_train:")
print(X_train.isna().sum())

# Fill missing values with column mean (or you could use other strategies like median or a fixed value)
X_train = X_train.fillna(X_train.mean())

# Check and handle NaNs in X_test similarly
print("Missing values in X_test:")
print(X_test.isna().sum())
X_test = X_test.fillna(X_test.mean())

feature_model.fit(X_train, y_train)

feature_importance = feature_model.feature_importances_

feature_importance

feature_importance_df_train = pd.DataFrame({'Feature': X_train.columns, 'Importance': feature_importance})

feature_importance_df_train = feature_importance_df_train.sort_values(by='Importance', ascending=False)

top_features_list = feature_importance_df_train.head(7)["Feature"].tolist()

top_features_list

X_train_subset = X_train[top_features_list]
X_test_subset = X_test[top_features_list]

models = [
    ("Linear Regression", LinearRegression()),
    ("Decision Tree", DecisionTreeRegressor()),
    ("Random Forest", RandomForestRegressor(n_estimators=100, random_state=42)),
    ("xGB Boost", xgb.XGBRegressor(n_estimators=100, random_state=42)),
    ("Gradient Boost", GradientBoostingRegressor(n_estimators=100, random_state=42))
]

for name, model in models:
    # Train model
    model.fit(X_train_subset, y_train)

    # Cross-validation
    scores = cross_val_score(model, X_train_subset, y_train, cv=5)

    print(f"{name}: Cross-validation scores: {scores}")

for name, model in models:
    # Predict on test set
    y_pred = model.predict(X_test_subset)

    # Calculate MAE and RMSE
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))

    print(f"{name}: MAE = {mae}, RMSE = {rmse}")

random_forest = RandomForestRegressor()
random_forest_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10]
}

xgb_model = xgb.XGBRegressor()
xgb_params = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

gradient_boost = GradientBoostingRegressor()
gradient_boost_params = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7]
}

models_and_params = [
    (random_forest, random_forest_params, "Random Forest"),
    (xgb_model, xgb_params, "XGB Boost"),
    (gradient_boost, gradient_boost_params, "Gradient Boost")
]

for model, param_grid, name in models_and_params:
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=2)
    grid_search.fit(X_train_subset, y_train)

    best_params = grid_search.best_params_
    best_model = grid_search.best_estimator_

    print(f"\n{name} - Best Parameters: {best_params}")

# Evaluate on the test set
y_pred = best_model.predict(X_test_subset)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"{name} - MAE = {mae}, RMSE = {rmse}")

#best performing model
best_random_forest = RandomForestRegressor(n_estimators=200, max_depth=None, min_samples_split=2)
best_xgb_model = xgb.XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=7)
best_gradient_boost = GradientBoostingRegressor(n_estimators=200, learning_rate=0.1, max_depth=7)

voting_regressor = VotingRegressor(estimators=[
    ('random_forest', best_random_forest),
    ('xgb', best_xgb_model),
    ('gradient_boost', best_gradient_boost)
])

voting_regressor.fit(X_train_subset, y_train)

y_pred = voting_regressor.predict(X_test_subset)

mae_voting = mean_absolute_error(y_test, y_pred)
rmse_voting = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"Voting Ensemble Model: MAE = {mae_voting}, RMSE = {rmse_voting}")

X_players22 = players_22[X_train_subset.columns]

X_players22

X_players22 = pd.DataFrame(scaler.fit_transform(X_players22), columns=X_players22.columns)

X_test_22=X_players22[X_train_subset.columns]

y_pred_22 = voting_regressor.predict(X_test_22)

mae_22 = mean_absolute_error(y_test, y_pred_22)
rmse_22 = np.sqrt(mean_squared_error(y_test, y_pred_22))

print(f"MAE on players_22: {mae_22}")
print(f"RMSE on players_22: {rmse_22}")

data = pd.DataFrame({'Real': y_test, 'Predicted': y_pred_22})

sns.scatterplot(data=data, x='Real', y='Predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)
plt.xlabel('Real')
plt.ylabel('Predicted')
plt.show()

best_random_forest.fit(X_train_subset, y_train)

y_pred_22 = best_random_forest.predict(X_test_22)

mae_22 = mean_absolute_error(y_test, y_pred_22)
rmse_22 = np.sqrt(mean_squared_error(y_test, y_pred_22))

print(f"MAE on players_22: {mae_22}")
print(f"RMSE on players_22: {rmse_22}")

data = pd.DataFrame({'Real': y_test, 'Predicted': y_pred_22})

sns.scatterplot(data=data, x='Real', y='Predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)
plt.xlabel('Real')
plt.ylabel('Predicted')
plt.show()

best_xgb_model.fit(X_train_subset, y_train)

y_pred_22 = best_xgb_model.predict(X_test_22)

mae_22 = mean_absolute_error(y_test, y_pred_22)
rmse_22 = np.sqrt(mean_squared_error(y_test, y_pred_22))

print(f"MAE on players_22: {mae_22}")
print(f"RMSE on players_22: {rmse_22}")

data = pd.DataFrame({'Real': y_test, 'Predicted': y_pred_22})

sns.scatterplot(data=data, x='Real', y='Predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)
plt.xlabel('Real')
plt.ylabel('Predicted')
plt.show()

best_gradient_boost.fit(X_train_subset, y_train)

y_pred_22 = best_gradient_boost.predict(X_test_22)

mae_22 = mean_absolute_error(y_test, y_pred_22)
rmse_22 = np.sqrt(mean_squared_error(y_test, y_pred_22))

print(f"MAE on players_22: {mae_22}")
print(f"RMSE on players_22: {rmse_22}")

data = pd.DataFrame({'Real': y_test, 'Predicted': y_pred_22})

sns.scatterplot(data=data, x='Real', y='Predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=3)
plt.xlabel('Real')
plt.ylabel('Predicted')
plt.show()

with open('top_features_list.pkl', 'wb') as file:
    pickle.dump(top_features_list, file)

joblib.dump(best_gradient_boost, "model.sav")

pickle.dump(best_gradient_boost, open("_model.sav", 'wb'))

joblib.dump(scaler,'scaler.pkl')

print(type(best_gradient_boost))

import os
cwd = os.getcwd()
print(cwd)

"""Gradient Boosting Regressor provides the best model"""